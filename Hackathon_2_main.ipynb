{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KcHVp-pbk7c"
      },
      "source": [
        "# Hackathon 2 — SDXL + CLIP Reranking\n",
        "\n",
        "This notebook generates images with **SDXL Base** (optionally **Refiner**) or **SD 3.5 Large**, scores the candidates with **CLIP**, and selects the best match.\n",
        "\n",
        "### Modes\n",
        "- **Mode A:** use your prompt as-is.\n",
        "- **Mode B (optional):** an LLM (via vLLM) converts a long text/article into a concise, SDXL-friendly prompt.\n",
        "\n",
        "### Quick tips\n",
        "- Switch `PRESET` to `\"fast\"` for iteration, `\"quality_1min\"` for ~1 minute per request, or `\"quality_max\"` for best quality.\n",
        "- If it's too slow, set `USE_REFINER = False`.\n",
        "- If you hit VRAM limits, try `HEIGHT/WIDTH = 768` and `n_images = 2`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdW2rJXPbk7e"
      },
      "outputs": [],
      "source": [
        "!pip install -q jedi vllm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dependencies for out of Collab Run\n",
        "# Install PyTorch with CUDA first\n",
        "# pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "# Then install other dependencies\n",
        "# pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "pZXptFti99N6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGa51VlGbk7f"
      },
      "source": [
        "## User Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Odm1Jtg-bk7f"
      },
      "outputs": [],
      "source": [
        "# ========== User settings ==========\n",
        "USE_VLLM = True           # True -> enable auto-prompt via vLLM\n",
        "USE_REFINER = True        # Refiner improves details but is slower\n",
        "PRESET = \"quality_1min\"    # \"fast\" | \"quality_1min\" | \"quality_max\"\n",
        "USE_TORCH_COMPILE = True # Set True for ~10-30% speedup (slower first run)\n",
        "\n",
        "OUT_DIR = \"images_out\"\n",
        "LOG_JSONL = \"images_log.jsonl\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PD0HxlDWbk7g"
      },
      "source": [
        "## Imports and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDM-zmq1bk7g"
      },
      "outputs": [],
      "source": [
        "import os, gc, json, re, time, uuid, unicodedata\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Dict, Any, List\n",
        "\n",
        "if USE_VLLM:\n",
        "    os.environ.setdefault(\"VLLM_WORKER_MULTIPROC_METHOD\", \"spawn\")\n",
        "    try:\n",
        "        import multiprocessing as mp\n",
        "        mp.set_start_method(\"spawn\", force=True)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, CLIPModel, CLIPProcessor\n",
        "from diffusers import DiffusionPipeline, StableDiffusion3Pipeline\n",
        "from PIL import Image\n",
        "\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "except Exception:\n",
        "    userdata = None\n",
        "\n",
        "if USE_VLLM:\n",
        "    from vllm import LLM as VLLM, SamplingParams\n",
        "\n",
        "print(\"Imports complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObGz3HNnbk7g"
      },
      "outputs": [],
      "source": [
        "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def gpu_total_gb() -> float:\n",
        "    if not torch.cuda.is_available():\n",
        "        return 0.0\n",
        "    return torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
        "\n",
        "def gpu_free_gb() -> float:\n",
        "    if not torch.cuda.is_available():\n",
        "        return 0.0\n",
        "    free, _ = torch.cuda.mem_get_info(0)\n",
        "    return free / (1024**3)\n",
        "\n",
        "def gpu_cleanup():\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "def get_gpu_metrics() -> Dict[str, float]:\n",
        "    if not torch.cuda.is_available():\n",
        "        return {}\n",
        "    try:\n",
        "        free, total = torch.cuda.mem_get_info(0)\n",
        "        return {\n",
        "            \"gpu_memory_used_gb\": round((total - free) / (1024**3), 2),\n",
        "            \"gpu_memory_total_gb\": round(total / (1024**3), 2),\n",
        "        }\n",
        "    except Exception:\n",
        "        return {}\n",
        "\n",
        "if DEVICE == \"cuda\":\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "    try:\n",
        "        torch.set_float32_matmul_precision(\"high\")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "print(f\"DEVICE: {DEVICE}\" + (f\" | GPU: {torch.cuda.get_device_name(0)} | VRAM: {gpu_total_gb():.1f} GB\" if DEVICE==\"cuda\" else \"\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYdY-xl_bk7h"
      },
      "outputs": [],
      "source": [
        "# Hugging Face token\n",
        "HF_TOKEN = None\n",
        "if userdata is not None:\n",
        "    try:\n",
        "        HF_TOKEN = userdata.get(\"HF_TOKEN\")\n",
        "    except Exception:\n",
        "        pass\n",
        "HF_TOKEN = HF_TOKEN or os.environ.get(\"HF_TOKEN\")\n",
        "\n",
        "if not HF_TOKEN:\n",
        "    print(\"⚠️  HF_TOKEN not found. Gated models may fail.\")\n",
        "else:\n",
        "    print(\"✅ HF_TOKEN loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xb7CwrScbk7h"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_EsJFZ-bk7h"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ImgCfg:\n",
        "    preset: str = \"\"\n",
        "    clip_min: float = 0.22\n",
        "    n_images: int = 2\n",
        "    seed_base: int = 42\n",
        "    steps_base: int = 35\n",
        "    steps_refine: int = 18\n",
        "    guidance: float = 7.0\n",
        "    high_noise_frac: float = 0.78\n",
        "    height: int = 1024\n",
        "    width: int = 1024\n",
        "    out_dir: str = OUT_DIR\n",
        "    log_jsonl: str = LOG_JSONL\n",
        "    save_images: bool = True\n",
        "    max_prompt_chars: int = 900\n",
        "    style_suffix: str = \", documentary photo, natural light, high detail, sharp focus\"\n",
        "    negative_prompt: str = \"text, watermark, logo, brand, nsfw, nude, gore, disfigured, low quality, blurry\"\n",
        "\n",
        "CFG = ImgCfg()\n",
        "os.makedirs(CFG.out_dir, exist_ok=True)\n",
        "\n",
        "def apply_preset(cfg: ImgCfg, preset: str) -> ImgCfg:\n",
        "    p = (preset or \"\").lower().strip()\n",
        "    cfg.preset = p\n",
        "    vram = gpu_total_gb()\n",
        "\n",
        "    if p == \"fast\":\n",
        "        cfg.n_images, cfg.steps_base, cfg.steps_refine = 2, 25, 10\n",
        "        cfg.guidance, cfg.clip_min = 6.5, 0.20\n",
        "        cfg.height = cfg.width = 768\n",
        "    elif p == \"quality_1min\":\n",
        "        cfg.n_images = 4 if vram >= 24 else 2\n",
        "        cfg.steps_base, cfg.steps_refine = 37, 20\n",
        "        cfg.guidance, cfg.clip_min = 7.0, 0.22\n",
        "        cfg.height = cfg.width = 1024\n",
        "    elif p == \"quality_max\":\n",
        "        cfg.n_images = 5 if vram >= 40 else (3 if vram >= 24 else 2)\n",
        "        cfg.steps_base, cfg.steps_refine = 40, 25\n",
        "        cfg.guidance, cfg.clip_min = 7.5, 0.23\n",
        "        cfg.height = cfg.width = 1024\n",
        "    return cfg\n",
        "\n",
        "CFG = apply_preset(CFG, PRESET)\n",
        "print(f\"CFG: {CFG}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1Q_37nlbk7h"
      },
      "source": [
        "## vLLM (Mode B) — Optional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fArmALK_bk7i"
      },
      "outputs": [],
      "source": [
        "llm, llm_tok, loaded_llm_id = None, None, None\n",
        "LLM_CANDIDATES = [\"Qwen/Qwen2.5-7B-Instruct\", \"meta-llama/Llama-3.1-8B-Instruct\"]\n",
        "\n",
        "def _choose_llm_dtype() -> str:\n",
        "    if not torch.cuda.is_available():\n",
        "        return \"float16\"\n",
        "    return \"bfloat16\" if torch.cuda.get_device_capability(0)[0] >= 8 else \"float16\"\n",
        "\n",
        "if USE_VLLM:\n",
        "    os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n",
        "    dtype = _choose_llm_dtype()\n",
        "    print(f\"vLLM dtype: {dtype}\")\n",
        "\n",
        "    for mid in LLM_CANDIDATES:\n",
        "        try:\n",
        "            llm_tok = AutoTokenizer.from_pretrained(mid, token=HF_TOKEN)\n",
        "            if llm_tok.pad_token is None:\n",
        "                llm_tok.pad_token = llm_tok.eos_token\n",
        "            # Llama needs special params for Colab compatibility\n",
        "            is_llama = \"llama\" in mid.lower()\n",
        "\n",
        "            extra_kwargs = {}\n",
        "            if is_llama:\n",
        "                extra_kwargs[\"enforce_eager\"] = True        # Disables CUDA graphs\n",
        "                extra_kwargs[\"tensor_parallel_size\"] = 1    # Single-GPU mode\n",
        "\n",
        "            llm = VLLM(\n",
        "                model=mid,\n",
        "                dtype=dtype,\n",
        "                max_model_len=2048,\n",
        "                gpu_memory_utilization=0.35,\n",
        "                disable_log_stats=True,\n",
        "                **extra_kwargs,\n",
        "            )\n",
        "            loaded_llm_id = mid\n",
        "            print(f\"✅ Loaded vLLM: {mid}\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ vLLM failed: {mid} -> {e}\")\n",
        "    if llm is None:\n",
        "        print(\"❌ No LLM loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AsdXv4Tbk7i"
      },
      "source": [
        "## Model Management\n",
        "Smart loading/unloading to prevent OOM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xqesUo5bk7i"
      },
      "outputs": [],
      "source": [
        "_sdxl_base, _sdxl_refiner, _sd35_pipe, _active_model = None, None, None, None\n",
        "\n",
        "SDXL_BASE_ID = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
        "SDXL_REFINER_ID = \"stabilityai/stable-diffusion-xl-refiner-1.0\"\n",
        "SD35_ID = \"stabilityai/stable-diffusion-3.5-large\"\n",
        "\n",
        "def _get_sd_dtype():\n",
        "    if DEVICE == \"cuda\" and torch.cuda.is_bf16_supported():\n",
        "        return torch.bfloat16\n",
        "    elif DEVICE == \"cuda\":\n",
        "        return torch.float16\n",
        "    return torch.float32\n",
        "\n",
        "DTYPE_SD = _get_sd_dtype()\n",
        "\n",
        "def _low_vram_mode() -> bool:\n",
        "    return DEVICE == \"cuda\" and gpu_total_gb() < 24\n",
        "\n",
        "def unload_diffusion_models(keep: str = None):\n",
        "    global _sdxl_base, _sdxl_refiner, _sd35_pipe, _active_model\n",
        "    if keep != \"sdxl\":\n",
        "        for m in [_sdxl_base, _sdxl_refiner]:\n",
        "            if m is not None:\n",
        "                m.to(\"cpu\")\n",
        "                del m\n",
        "        _sdxl_base = _sdxl_refiner = None\n",
        "    if keep != \"sd35\" and _sd35_pipe is not None:\n",
        "        _sd35_pipe.to(\"cpu\")\n",
        "        del _sd35_pipe\n",
        "        _sd35_pipe = None\n",
        "    if keep is None:\n",
        "        _active_model = None\n",
        "    gpu_cleanup()\n",
        "\n",
        "def load_sdxl(use_refiner: bool = True):\n",
        "    global _sdxl_base, _sdxl_refiner, _active_model\n",
        "    if _sdxl_base is not None and (_sdxl_refiner is not None or not use_refiner):\n",
        "        return _sdxl_base, _sdxl_refiner\n",
        "    if _active_model == \"sd35\":\n",
        "        print(\"Unloading SD3.5...\")\n",
        "        unload_diffusion_models(keep=\"sdxl\")\n",
        "    gpu_cleanup()\n",
        "\n",
        "    print(\"Loading SDXL Base...\")\n",
        "    _sdxl_base = DiffusionPipeline.from_pretrained(\n",
        "        SDXL_BASE_ID, torch_dtype=DTYPE_SD,\n",
        "        variant=\"fp16\" if DTYPE_SD == torch.float16 else None,\n",
        "        use_safetensors=True, token=HF_TOKEN\n",
        "    ).to(DEVICE)\n",
        "    _sdxl_base.set_progress_bar_config(disable=True)\n",
        "    if _low_vram_mode():\n",
        "        _sdxl_base.enable_attention_slicing()\n",
        "        _sdxl_base.enable_vae_slicing()\n",
        "\n",
        "    # Optional: Compile UNet for faster inference\n",
        "    if USE_TORCH_COMPILE and hasattr(torch, \"compile\"):\n",
        "      try:\n",
        "          _sdxl_base.unet = torch.compile(_sdxl_base.unet, mode=\"reduce-overhead\")\n",
        "          print(\"✅ SDXL Base UNet compiled\")\n",
        "      except Exception as e:\n",
        "          print(f\"⚠️ Compilation skipped: {e}\")\n",
        "\n",
        "    if use_refiner:\n",
        "        print(\"Loading SDXL Refiner...\")\n",
        "        _sdxl_refiner = DiffusionPipeline.from_pretrained(\n",
        "            SDXL_REFINER_ID, torch_dtype=DTYPE_SD,\n",
        "            variant=\"fp16\" if DTYPE_SD == torch.float16 else None,\n",
        "            use_safetensors=True, token=HF_TOKEN\n",
        "        ).to(DEVICE)\n",
        "        _sdxl_refiner.set_progress_bar_config(disable=True)\n",
        "        if _low_vram_mode():\n",
        "            _sdxl_refiner.enable_attention_slicing()\n",
        "            _sdxl_refiner.enable_vae_slicing()\n",
        "\n",
        "        if USE_TORCH_COMPILE and hasattr(torch, \"compile\"):\n",
        "          try:\n",
        "              _sdxl_refiner.unet = torch.compile(_sdxl_refiner.unet, mode=\"reduce-overhead\")\n",
        "              print(\"✅ SDXL Refiner UNet compiled\")\n",
        "          except Exception as e:\n",
        "              print(f\"⚠️ Compilation skipped: {e}\")\n",
        "\n",
        "    _active_model = \"sdxl\"\n",
        "    print(\"✅ SDXL ready.\")\n",
        "    return _sdxl_base, _sdxl_refiner\n",
        "\n",
        "def load_sd35():\n",
        "    global _sd35_pipe, _active_model\n",
        "    if _sd35_pipe is not None:\n",
        "        return _sd35_pipe\n",
        "    if _active_model == \"sdxl\":\n",
        "        print(\"Unloading SDXL...\")\n",
        "        unload_diffusion_models(keep=\"sd35\")\n",
        "    gpu_cleanup()\n",
        "\n",
        "    print(\"Loading SD 3.5 Large...\")\n",
        "    _sd35_pipe = StableDiffusion3Pipeline.from_pretrained(\n",
        "        SD35_ID, torch_dtype=DTYPE_SD, token=HF_TOKEN\n",
        "    ).to(DEVICE)\n",
        "    _sd35_pipe.set_progress_bar_config(disable=True)\n",
        "\n",
        "    # Optional: Compile transformer for faster inference\n",
        "    if USE_TORCH_COMPILE and hasattr(torch, \"compile\"):\n",
        "      try:\n",
        "        _sd35_pipe.transformer = torch.compile(_sd35_pipe.transformer, mode=\"reduce-overhead\")\n",
        "        print(\"✅ SD3.5 transformer compiled\")\n",
        "      except Exception as e:\n",
        "        print(f\"⚠️ Compilation skipped: {e}\")\n",
        "    _active_model = \"sd35\"\n",
        "    print(\"✅ SD 3.5 ready.\")\n",
        "\n",
        "    return _sd35_pipe\n",
        "\n",
        "print(\"Model management ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6sXOM1Nbk7i"
      },
      "source": [
        "## Generation Functions\n",
        "With fixed generator handling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yug19Qblbk7j"
      },
      "outputs": [],
      "source": [
        "def sdxl_generate_single(prompt: str, seed: int, negative_prompt: str, cfg: ImgCfg, use_refiner: bool = True) -> Image.Image:\n",
        "    base, refiner = load_sdxl(use_refiner=use_refiner)\n",
        "\n",
        "    if use_refiner and refiner is not None:\n",
        "        g_base = torch.Generator(device=DEVICE).manual_seed(int(seed))\n",
        "        latents = base(\n",
        "            prompt=prompt, negative_prompt=negative_prompt or None,\n",
        "            num_inference_steps=cfg.steps_base, guidance_scale=cfg.guidance,\n",
        "            generator=g_base, height=cfg.height, width=cfg.width, output_type=\"latent\"\n",
        "        ).images\n",
        "\n",
        "        # FIXED: Fresh generator for refiner\n",
        "        g_refine = torch.Generator(device=DEVICE).manual_seed(int(seed))\n",
        "        return refiner(\n",
        "            prompt=prompt, negative_prompt=negative_prompt or None,\n",
        "            num_inference_steps=cfg.steps_refine, guidance_scale=cfg.guidance,\n",
        "            generator=g_refine, image=latents, strength=1.0 - float(cfg.high_noise_frac)\n",
        "        ).images[0]\n",
        "\n",
        "    g = torch.Generator(device=DEVICE).manual_seed(int(seed))\n",
        "    return base(\n",
        "        prompt=prompt, negative_prompt=negative_prompt or None,\n",
        "        num_inference_steps=cfg.steps_base, guidance_scale=cfg.guidance,\n",
        "        generator=g, height=cfg.height, width=cfg.width, output_type=\"pil\"\n",
        "    ).images[0]\n",
        "\n",
        "def sdxl_generate_batch(prompt: str, seeds: List[int], negative_prompt: str, cfg: ImgCfg, use_refiner: bool = True) -> List[Image.Image]:\n",
        "    base, refiner = load_sdxl(use_refiner=use_refiner)\n",
        "    seeds = [int(s) for s in seeds]\n",
        "    n = len(seeds)\n",
        "    prompts, negs = [prompt] * n, [negative_prompt] * n if negative_prompt else None\n",
        "\n",
        "    if use_refiner and refiner is not None:\n",
        "        gens_base = [torch.Generator(device=DEVICE).manual_seed(s) for s in seeds]\n",
        "        latents = base(\n",
        "            prompt=prompts, negative_prompt=negs,\n",
        "            num_inference_steps=cfg.steps_base, guidance_scale=cfg.guidance,\n",
        "            generator=gens_base, height=cfg.height, width=cfg.width, output_type=\"latent\"\n",
        "        ).images\n",
        "\n",
        "        # FIXED: Fresh generators for refiner\n",
        "        gens_refine = [torch.Generator(device=DEVICE).manual_seed(s) for s in seeds]\n",
        "        return refiner(\n",
        "            prompt=prompts, negative_prompt=negs,\n",
        "            num_inference_steps=cfg.steps_refine, guidance_scale=cfg.guidance,\n",
        "            generator=gens_refine, image=latents, strength=1.0 - float(cfg.high_noise_frac)\n",
        "        ).images\n",
        "\n",
        "    gens = [torch.Generator(device=DEVICE).manual_seed(s) for s in seeds]\n",
        "    return base(\n",
        "        prompt=prompts, negative_prompt=negs,\n",
        "        num_inference_steps=cfg.steps_base, guidance_scale=cfg.guidance,\n",
        "        generator=gens, height=cfg.height, width=cfg.width, output_type=\"pil\"\n",
        "    ).images\n",
        "\n",
        "print(\"SDXL functions defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQeVYGLDbk7j"
      },
      "outputs": [],
      "source": [
        "# NEW: SD 3.5 batch generation\n",
        "def sd35_generate_single(prompt: str, seed: int, negative_prompt: str, cfg: ImgCfg, use_refiner: bool = False) -> Image.Image:\n",
        "    pipe = load_sd35()\n",
        "    gen = torch.Generator(device=DEVICE).manual_seed(int(seed))\n",
        "    return pipe(\n",
        "        prompt=prompt, negative_prompt=negative_prompt,\n",
        "        height=cfg.height, width=cfg.width,\n",
        "        num_inference_steps=cfg.steps_base, guidance_scale=cfg.guidance, generator=gen\n",
        "    ).images[0]\n",
        "\n",
        "def sd35_generate_batch(prompt: str, seeds: List[int], negative_prompt: str, cfg: ImgCfg, use_refiner: bool = False) -> List[Image.Image]:\n",
        "    pipe = load_sd35()\n",
        "    seeds = [int(s) for s in seeds]\n",
        "    n = len(seeds)\n",
        "    prompts, negs = [prompt] * n, [negative_prompt] * n if negative_prompt else None\n",
        "    gens = [torch.Generator(device=DEVICE).manual_seed(s) for s in seeds]\n",
        "    return pipe(\n",
        "        prompt=prompts, negative_prompt=negs,\n",
        "        height=cfg.height, width=cfg.width,\n",
        "        num_inference_steps=cfg.steps_base, guidance_scale=cfg.guidance, generator=gens\n",
        "    ).images\n",
        "\n",
        "print(\"SD 3.5 functions defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkemsyusbk7j"
      },
      "outputs": [],
      "source": [
        "# Router\n",
        "def generate_single(prompt: str, seed: int, negative_prompt: str, cfg: ImgCfg, use_refiner: bool = True) -> Image.Image:\n",
        "    if cfg.preset == \"quality_max\":\n",
        "        return sd35_generate_single(prompt, seed, negative_prompt, cfg)\n",
        "    return sdxl_generate_single(prompt, seed, negative_prompt, cfg, use_refiner)\n",
        "\n",
        "def generate_batch(prompt: str, seeds: List[int], negative_prompt: str, cfg: ImgCfg, use_refiner: bool = True) -> List[Image.Image]:\n",
        "    if cfg.preset == \"quality_max\":\n",
        "        return sd35_generate_batch(prompt, seeds, negative_prompt, cfg)\n",
        "    return sdxl_generate_batch(prompt, seeds, negative_prompt, cfg, use_refiner)\n",
        "\n",
        "print(f\"Router: '{PRESET}' -> {'SD3.5' if PRESET == 'quality_max' else 'SDXL'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLuQhUtubk7j"
      },
      "source": [
        "## CLIP Scoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuQJbqzMbk7j"
      },
      "outputs": [],
      "source": [
        "CLIP_ID = \"openai/clip-vit-large-patch14\"\n",
        "_clip_model, _clip_processor, _clip_tokenizer = None, None, None\n",
        "\n",
        "def get_clip_tokenizer():\n",
        "    global _clip_tokenizer\n",
        "    if _clip_tokenizer is None:\n",
        "        _clip_tokenizer = CLIPProcessor.from_pretrained(CLIP_ID).tokenizer\n",
        "    return _clip_tokenizer\n",
        "\n",
        "def load_clip():\n",
        "    global _clip_model, _clip_processor\n",
        "    if _clip_model is not None:\n",
        "        return _clip_model, _clip_processor\n",
        "    gpu_cleanup()\n",
        "    _clip_model = CLIPModel.from_pretrained(CLIP_ID).to(DEVICE).eval()\n",
        "    _clip_processor = CLIPProcessor.from_pretrained(CLIP_ID)\n",
        "    return _clip_model, _clip_processor\n",
        "\n",
        "@torch.inference_mode()\n",
        "def clip_score_batch(text: str, images: List[Image.Image]) -> List[float]:\n",
        "    model, processor = load_clip()\n",
        "    text = \" \".join((text or \"\").strip().split())\n",
        "    if not text or not images:\n",
        "        return []\n",
        "\n",
        "    t_inputs = processor(text=[text], return_tensors=\"pt\", padding=True, truncation=True, max_length=77).to(DEVICE)\n",
        "    i_inputs = processor(images=images, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "    text_feat = model.get_text_features(input_ids=t_inputs[\"input_ids\"], attention_mask=t_inputs.get(\"attention_mask\"))\n",
        "    img_feat = model.get_image_features(pixel_values=i_inputs[\"pixel_values\"])\n",
        "\n",
        "    text_feat = text_feat / text_feat.norm(dim=-1, keepdim=True)\n",
        "    img_feat = img_feat / img_feat.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    return (img_feat @ text_feat.T).squeeze(-1).float().cpu().tolist()\n",
        "\n",
        "print(\"✅ CLIP ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyprNYPcbk7k"
      },
      "source": [
        "## Prompt Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjE6a6aYbk7k"
      },
      "outputs": [],
      "source": [
        "def normalize_text(text: str) -> str:\n",
        "    text = unicodedata.normalize(\"NFKC\", text)\n",
        "    text = re.sub(r\"[\\u200b-\\u200f\\u2060\\ufeff]\", \"\", text)\n",
        "    return \" \".join(text.lower().split())\n",
        "\n",
        "def basic_prompt_filter(prompt: str) -> Dict[str, Any]:\n",
        "    p = normalize_text(prompt or \"\")\n",
        "    banned = [r\"n[s$]fw\", r\"nud[e3]\", r\"nudity\", r\"p[o0]rn\", r\"s[e3]x\", r\"g[o0]r[e3]\", r\"explicit\", r\"xxx\", r\"naked\", r\"erotic\"]\n",
        "    hits = set()\n",
        "    for pat in banned:\n",
        "        for m in re.finditer(rf\"\\b{pat}\\b\", p):\n",
        "            prev = p[max(0, m.start()-4):m.start()]\n",
        "            if not prev.endswith(\"no \"):\n",
        "                hits.add(m.group())\n",
        "    return {\"safe\": len(hits) == 0, \"hits\": sorted(hits)}\n",
        "\n",
        "def normalize_for_generation(prompt: str, cfg: ImgCfg) -> str:\n",
        "    p = re.sub(r\"\\s+\", \" \", (prompt or \"\").strip())\n",
        "    if not p:\n",
        "        return \"\"\n",
        "    if cfg.style_suffix and cfg.style_suffix.lower() not in p.lower():\n",
        "        p = (p + cfg.style_suffix).strip()\n",
        "    return p[:cfg.max_prompt_chars]\n",
        "\n",
        "def make_clip_prompt(prompt: str, cfg: ImgCfg, max_tokens: int = 77) -> str:\n",
        "    tok = get_clip_tokenizer()\n",
        "    p = re.sub(r\"\\s+\", \" \", (prompt or \"\").strip())\n",
        "    if cfg.style_suffix:\n",
        "        suf_lower = cfg.style_suffix.lower()\n",
        "        if suf_lower in p.lower():\n",
        "            idx = p.lower().find(suf_lower)\n",
        "            p = (p[:idx] + p[idx + len(cfg.style_suffix):]).strip(\" ,\")\n",
        "    p = \" \".join(p.split()[:40])\n",
        "    ids = tok(p, add_special_tokens=True, truncation=False)[\"input_ids\"]\n",
        "    if len(ids) > max_tokens:\n",
        "        p = tok.decode(ids[:max_tokens], skip_special_tokens=True).strip()\n",
        "    return p\n",
        "\n",
        "def auto_prompt_from_text(text: str) -> str:\n",
        "    if not USE_VLLM or llm is None:\n",
        "        raise RuntimeError(\"LLM not loaded\")\n",
        "    system = \"You are a prompt-writer for an image model.\"\n",
        "    user = f\"Turn the text into ONE short visual prompt for a documentary photo. Focus on concrete scene. Output only the prompt.\\n\\nTEXT:\\n{text.strip()}\"\n",
        "    sp = SamplingParams(temperature=0.6, top_p=0.9, max_tokens=120)\n",
        "    msgs = [{\"role\": \"system\", \"content\": system}, {\"role\": \"user\", \"content\": user}]\n",
        "    prompt = llm_tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
        "    out = llm.generate([prompt], sp)[0].outputs[0].text\n",
        "    print(f\"\\n{'═'*60}\\n{'AUTO-PROMPT':^60}\\n{'═'*60}\\n{out.strip()}\\n{'═'*60}\\n\")\n",
        "    return re.sub(r\"\\s+\", \" \", out).strip()\n",
        "\n",
        "print(\"Prompt utilities ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mU5nXjzlbk7k"
      },
      "source": [
        "## Logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UOml1Bnbk7k"
      },
      "outputs": [],
      "source": [
        "def ts_id() -> str:\n",
        "    return time.strftime(\"%Y%m%d_%H%M%S\") + \"_\" + uuid.uuid4().hex[:6]\n",
        "\n",
        "def write_log(rec: Dict[str, Any], path: str):\n",
        "    rec[\"gpu_metrics\"] = get_gpu_metrics()\n",
        "    rec[\"timestamp\"] = time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime())\n",
        "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
        "        f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(\"Logging ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bsPfMCwbk7k"
      },
      "source": [
        "## Main Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_CsgWwabk7k"
      },
      "outputs": [],
      "source": [
        "def run_image_pipeline(*, mode: str, user_prompt: Optional[str] = None, text: Optional[str] = None,\n",
        "                        item_id: Optional[str] = None, base_seed: Optional[int] = None,\n",
        "                        cfg: ImgCfg = CFG, use_refiner: bool = USE_REFINER,\n",
        "                        return_candidates_images: bool = False) -> Dict[str, Any]:\n",
        "    assert mode in {\"user_prompt\", \"auto_prompt\"}\n",
        "    item_id = item_id or ts_id()\n",
        "    base_seed = cfg.seed_base if base_seed is None else int(base_seed)\n",
        "\n",
        "    # 1) Get prompt\n",
        "    if mode == \"user_prompt\":\n",
        "        raw = (user_prompt or \"\").strip()\n",
        "        if not raw:\n",
        "            rec = {\"ok\": False, \"reason\": \"missing_user_prompt\", \"mode\": mode, \"item_id\": item_id}\n",
        "            write_log(rec, cfg.log_jsonl)\n",
        "            return rec\n",
        "    else:\n",
        "        if not (text or \"\").strip():\n",
        "            rec = {\"ok\": False, \"reason\": \"missing_text\", \"mode\": mode, \"item_id\": item_id}\n",
        "            write_log(rec, cfg.log_jsonl)\n",
        "            return rec\n",
        "        try:\n",
        "            raw = auto_prompt_from_text(text)\n",
        "        except Exception as e:\n",
        "            rec = {\"ok\": False, \"reason\": \"auto_prompt_failed\", \"err\": str(e)[:300], \"mode\": mode, \"item_id\": item_id}\n",
        "            write_log(rec, cfg.log_jsonl)\n",
        "            return rec\n",
        "\n",
        "    # 2) Filter\n",
        "    f = basic_prompt_filter(raw)\n",
        "    if not f[\"safe\"]:\n",
        "        rec = {\"ok\": False, \"reason\": \"blocked_prompt\", \"hits\": f[\"hits\"], \"mode\": mode, \"item_id\": item_id}\n",
        "        write_log(rec, cfg.log_jsonl)\n",
        "        return rec\n",
        "\n",
        "    sd_prompt = normalize_for_generation(raw, cfg)\n",
        "    clip_prompt = make_clip_prompt(sd_prompt, cfg)\n",
        "    if not sd_prompt or not clip_prompt:\n",
        "        rec = {\"ok\": False, \"reason\": \"empty_prompt\", \"mode\": mode, \"item_id\": item_id}\n",
        "        write_log(rec, cfg.log_jsonl)\n",
        "        return rec\n",
        "\n",
        "    # 3) Generate\n",
        "    seeds = [base_seed + i for i in range(cfg.n_images)]\n",
        "    candidates = [{\"seed\": int(s)} for s in seeds]\n",
        "    ok_imgs, ok_indices = [], []\n",
        "\n",
        "    t0 = time.time()\n",
        "    try:\n",
        "        imgs = generate_batch(sd_prompt, seeds, cfg.negative_prompt, cfg, use_refiner)\n",
        "        ok_imgs, ok_indices = imgs, list(range(len(seeds)))\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Batch failed: {e}, trying sequential...\")\n",
        "        for idx, s in enumerate(seeds):\n",
        "            try:\n",
        "                ok_imgs.append(generate_single(sd_prompt, s, cfg.negative_prompt, cfg, use_refiner))\n",
        "                ok_indices.append(idx)\n",
        "            except Exception as e2:\n",
        "                candidates[idx][\"error\"] = str(e2)[:200]\n",
        "    gen_s = time.time() - t0\n",
        "\n",
        "    if not ok_imgs:\n",
        "        rec = {\"ok\": False, \"reason\": \"generation_failed\", \"mode\": mode, \"item_id\": item_id,\n",
        "               \"sd_prompt\": sd_prompt, \"candidates\": candidates}\n",
        "        write_log(rec, cfg.log_jsonl)\n",
        "        return rec\n",
        "\n",
        "    # 4) CLIP score\n",
        "    t1 = time.time()\n",
        "    scores = clip_score_batch(clip_prompt, ok_imgs)\n",
        "    clip_s = time.time() - t1\n",
        "\n",
        "    for idx, sc in zip(ok_indices, scores):\n",
        "        candidates[idx][\"clip\"] = float(sc)\n",
        "\n",
        "    best_local = max(range(len(scores)), key=lambda i: scores[i])\n",
        "    best_clip, best_idx = float(scores[best_local]), ok_indices[best_local]\n",
        "    best_seed, best_img = seeds[best_idx], ok_imgs[best_local]\n",
        "\n",
        "    if best_clip < cfg.clip_min:\n",
        "        rec = {\"ok\": False, \"reason\": \"low_clip_alignment\", \"best_clip\": best_clip, \"clip_min\": cfg.clip_min,\n",
        "               \"mode\": mode, \"item_id\": item_id, \"candidates\": candidates}\n",
        "        write_log(rec, cfg.log_jsonl)\n",
        "        return rec\n",
        "\n",
        "    # 5) Save\n",
        "    img_path = None\n",
        "    if cfg.save_images:\n",
        "        img_path = os.path.join(cfg.out_dir, f\"{item_id}_seed{best_seed}_clip{best_clip:.3f}.png\")\n",
        "        best_img.save(img_path)\n",
        "\n",
        "    # Log record (without images - they're not JSON serializable)\n",
        "    log_rec = {\"ok\": True, \"mode\": mode, \"item_id\": item_id, \"sd_prompt\": sd_prompt, \"clip_prompt\": clip_prompt,\n",
        "               \"best_seed\": best_seed, \"best_clip\": best_clip, \"img_path\": img_path,\n",
        "               \"timing_s\": {\"gen\": round(gen_s, 2), \"clip\": round(clip_s, 2)}, \"candidates\": candidates, \"preset\": cfg.preset}\n",
        "    write_log(log_rec, cfg.log_jsonl)\n",
        "\n",
        "    # Return record (with images if requested)\n",
        "    result = log_rec.copy()\n",
        "    if return_candidates_images:\n",
        "        result[\"candidate_images\"] = ok_imgs\n",
        "        result[\"best_image\"] = best_img\n",
        "\n",
        "    return result\n",
        "\n",
        "print(\"✅ Pipeline ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvVW7taYbk7l"
      },
      "source": [
        "## Convenience Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKilbajcbk7l"
      },
      "outputs": [],
      "source": [
        "def generate_image(mode: str = \"user_prompt\", prompt: str = \"\", text: str = \"\", show: bool = True, **kwargs) -> Dict[str, Any]:\n",
        "    # Use return_candidates_images from kwargs if provided, otherwise use show\n",
        "    return_images = kwargs.pop(\"return_candidates_images\", show)\n",
        "\n",
        "    result = run_image_pipeline(mode=mode, user_prompt=prompt if mode == \"user_prompt\" else None,\n",
        "                                 text=text if mode == \"auto_prompt\" else None, return_candidates_images=return_images, **kwargs)\n",
        "\n",
        "    if show and result.get(\"ok\") and \"best_image\" in result:\n",
        "        display(result[\"best_image\"])\n",
        "        print(f\"\\n✅ Best: seed={result['best_seed']}, CLIP={result['best_clip']:.4f}\")\n",
        "        print(f\"   Timing: gen={result['timing_s']['gen']:.1f}s, clip={result['timing_s']['clip']:.2f}s\")\n",
        "        if result.get(\"img_path\"):\n",
        "            print(f\"   Saved: {result['img_path']}\")\n",
        "    elif not result.get(\"ok\"):\n",
        "        print(f\"❌ Failed: {result.get('reason', 'unknown')}\")\n",
        "        if \"hits\" in result:\n",
        "            print(f\"   Blocked: {result['hits']}\")\n",
        "    return result\n",
        "\n",
        "print(\"✅ Ready! Use generate_image() to create images.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTKXJ3QZbk7l"
      },
      "source": [
        "## Examples\\n### Mode A: Direct Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PG6xebkVbk7l"
      },
      "outputs": [],
      "source": [
        "# Example: Direct prompt\n",
        "result = generate_image(prompt=\"A golden retriever puppy playing in autumn leaves, warm afternoon sunlight\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIBdYAz7bk7l"
      },
      "source": [
        "### Mode B: Auto-Prompt from Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0fRW26ibk7l",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Example: Auto-prompt from long text\n",
        "text = '''\n",
        "  Climate scientists have observed unprecedented changes in Arctic ice patterns.\n",
        "  The polar regions are experiencing warming at twice the global average rate,\n",
        "  leading to significant changes in wildlife habitats and migration patterns.\n",
        "'''\n",
        "\n",
        "result = generate_image(mode=\"auto_prompt\", text=text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N30ezK92bk7m"
      },
      "source": [
        "### Show All Candidates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idjMY2_Zbk7m",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Show all candidates\n",
        "result = generate_image(prompt=\"A cozy coffee shop interior with morning light\", show=False, return_candidates_images=True)\n",
        "\n",
        "if result.get(\"ok\") and \"candidate_images\" in result:\n",
        "    import matplotlib.pyplot as plt\n",
        "    n = len(result[\"candidate_images\"])\n",
        "    fig, axes = plt.subplots(1, n, figsize=(5*n, 5))\n",
        "    if n == 1: axes = [axes]\n",
        "    for i, (img, cand) in enumerate(zip(result[\"candidate_images\"], result[\"candidates\"])):\n",
        "        axes[i].imshow(img)\n",
        "        axes[i].set_title(f\"Seed {cand['seed']}\\nCLIP: {cand.get('clip', 0):.4f}\")\n",
        "        axes[i].axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print(f\"\\n✅ Best: seed={result['best_seed']}, CLIP={result['best_clip']:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}